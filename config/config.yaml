
#I usually have this included so that my 'test/debugging' runs of the code don't clutter the repo or my proper experiment logging folder with outputs.
hydra:
  run:
    dir: /raid/dlgroupmsc/${now:%Y-%m-%d_%H-%M-%S}
#Note that config hierarchy introduces the design problem of deciding where config options should reside :)
#For example when working with EO data, is the crop size dataset dependent or not?
defaults:
  - _self_
  - dataset: read_dataset
  - transform: basic

seed: 42
device: cuda:2 #cuda:0 #or cpu, or cuda:# on machines with more than one GPU
use_transform: True
batch_size: 32
eval_every: 10
save_model_freq: 500 #Don't do this to often, max a couple of times over a couple of thousand epochs. If no intention of warm starting training it is not needed at all.
val_batch_size: 8 #I typically set this as large as VRAM allows
num_workers: 6 #Set this based on machine, I think it's best to max it, usually faster training that way.
lr: 0.0002
max_epochs: 100
save_optimizer: False #Set this to true if you want to be able to keep training the model at a later stage, as of now the code has no support for this.
weight_decay: 0
optimizer: adam
beta1: 0.9
momentum: 0.9

model:
  pretrained: DeepLabV3_ResNet50_Weights.DEFAULT #None does not work, but they seem to want it
  pretrained_backbone: ResNet50_Weights.IMAGENET1K_V2 #default, can also use enum, read docs
  n_class: 19 #13 for grouped, 19 for all
  n_channels: 5

