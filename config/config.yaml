
#I usually have this included so that my 'test/debugging' runs of the code don't clutter the repo or my proper experiment logging folder with outputs.
hydra:
  run:
    dir: ../sink #/../sink/${now:%Y-%m-%d_%H-%M-%S}
  

#Note that config hierarchy introduces the design problem of deciding where config options should reside :)
#For example when working with EO data, is the crop size dataset dependent or not? 
defaults:
  - _self_
  - dataset: read_set
  - transform: basic

#now I have all settings here instead, it is kind of strange


seed: 42
device: cpu #cuda:0 #or cpu, or cuda:# on machines with more than one GPU
use_transform: False #should be true but have not implemented transforms
batch_size: 16
eval_every: 10
save_model_freq: 500 #Don't do this to often, max a couple of times over a couple of thousand epochs. If no intention of warm starting training it is not needed at all.
val_batch_size: 16 #I typically set this as large as VRAM allows
num_workers: 1 #Set this based on machine, I think it's best to max it, usually faster training that way, was 16
#crop_size: 512
lr: 0.0002
max_epochs: 1000
save_optimizer: False #Set this to true if you want to be able to keep training the model at a later stage, as of now the code has no support for this.
weight_decay: 0
optimizer: adam
beta1: 0.9 #first moment in adam optimizer

model:
  pretrained: False
  pretrained_backbone: True
  n_class: 20 #for flair dataset (really 13, but don't know with indexing, might need a class called 13)
  n_channels: 5 #for flair data_set


