
#I usually have this included so that my 'test/debugging' runs of the code don't clutter the repo or my proper experiment logging folder with outputs.
hydra:
  run:
    dir: ../log_res/${now:%Y-%m-%d_%H-%M-%S}

#Note that config hierarchy introduces the design problem of deciding where config options should reside :)
#For example when working with EO data, is the crop size dataset dependent or not?
defaults:
  - _self_
  - dataset: read_dataset
  - transform: basic

seed: 42 #42
device: cuda:0 #cuda:0 #or cpu, or cuda:# on machines with more than one GPU
use_transform: True
batch_size: 8
eval_every: 1
eval_type: image_wise_fade
save_model_freq: 1000 #Don't do this to often, max a couple of times over a couple of thousand epochs. If no intention of warm starting training it is not needed at all.
val_batch_size: 2 #I typically set this as large as VRAM allows
test_batch_size: 2 # how large should this be?
num_workers: 10 #Set this based on machine, I think it's best to max it, usually faster training that way.
lr: 0.0002 #0.0002 for CE
max_epochs: 300
save_optimizer: False #Set this to true if you want to be able to keep training the model at a later stage, as of now the code has no support for this.
weight_decay: 0
optimizer: adam
loss_function: teacher_student_loss #senti_loss #tversky
beta1: 0.9
momentum: 0.9

model:
  name: teacher_student
  pretrained: DeepLabV3_ResNet50_Weights.DEFAULT #None does not work, but they seem to want it
  pretrained_backbone: ResNet50_Weights.IMAGENET1K_V2 #default, can also use enum, read docs. ResNet50_Weights.IMAGENET1K_V2 are improved from ...1K_V1 
  n_class: 19 #13 for grouped, 19 for all but should be for training so set 19
  n_channels: 5
  teacher_student: 
    student_name: unet
    teacher_path: '../log_res/2024-02-08_17-09-25'
    teacher_type: 'unet'
    teacher_weight: 0.4
    teacher_channels: 5
    student_channels: 3
    ts_loss: KL

